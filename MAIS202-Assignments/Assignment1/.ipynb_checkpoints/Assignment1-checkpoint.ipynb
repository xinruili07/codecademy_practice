{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpfSxrhzAx-M"
   },
   "source": [
    "# MAIS 202 Fall 2019 - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CEouScBAx-N"
   },
   "source": [
    "Welcome to the MAIS 202 ML bootcamp! Get ready for an amazing 9 weeks, where you'll learn ML fundamentals, make new friends, and become a better developer.\n",
    "\n",
    "In this first assignment, we will be implementing the first concept taught in every ML class: linear regression. Specifically, we will be solidifying your knowledge of linear regression, gradient descent, train-val-test split, overfitting/underfitting, and L2 regularization. While you will likely never have to implement your own linear regression algorithm from scratch in practice, doing so will help you better understand the underlying mathematics behind the concepts.\n",
    "\n",
    "* [Question 1](#scrollTo=yPa8GRh4Ax-b)\n",
    "* [Question 2](#scrollTo=7OANFdRrAx-i)\n",
    "* [Question 3](#scrollTo=DKeqX2vgAx_d)\n",
    "* [Question 4](#scrollTo=HCqVvsNsSmSo)\n",
    "* [Question 5](#scrollTo=6LygvqAhAx_j)\n",
    "\n",
    "For those of you who've never used Jupyter/Collab notebooks before, simply press `Ctrl` + `Enter` to run each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cXRPuBxAx-O"
   },
   "outputs": [],
   "source": [
    "# we will start by installing then importing the relevant Python libraries\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t44I258SAx-R"
   },
   "source": [
    "## 1) Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AZEKHQ-Ax-R"
   },
   "source": [
    "We're given three csv's for this assignment, `Dataset_1_train.csv`, `Dataset_1_valid.csv`, and `Dataset_1_test.csv`, which will be our training, validation, and test sets respectively. As mentioned in lectures, it's important to train our model on only a portion of the data available (the training set). Testing our model on data it has never seen gives us a better measure of the model's accuracy and tells us whether or not the model has overfit/underfit.\n",
    "\n",
    "The validation set is used during model development to 1) check the performance, bias (underfitting), and variance (overfitting) of the model, and 2) to tune hyperparameters. (We will be using the validation set for hyperparameter tuning later in the assignment.) Even though the model never trains on the validation set, our hyperparmaeters may still be tweaked in favour of validation set performance. As a result, we need the test set, which is only used at the very end of the ML pipeline, to measure the model's accuracy on truly unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1SCaIOGAx-S"
   },
   "outputs": [],
   "source": [
    "# read in the data as pandas dataframes\n",
    "data_train = pd.read_csv('https://raw.githubusercontent.com/cclin130/mais-202-assignment-2-f2019/master/Dataset_1_test.csv', header=None).sort_values(0)\n",
    "X_train = np.array(data_train.iloc[:,0])\n",
    "X_train = np.reshape(X_train, (len(X_train),1))\n",
    "y_train = np.array(data_train.iloc[:,1])\n",
    "y_train = np.reshape(y_train, (len(y_train),1))\n",
    "\n",
    "data_valid = pd.read_csv('https://raw.githubusercontent.com/cclin130/mais-202-assignment-2-f2019/master/Dataset_1_valid.csv', header=None).sort_values(0)\n",
    "X_valid = np.array(data_valid.iloc[:,0])\n",
    "X_valid = np.reshape(X_valid, (len(X_valid),1))\n",
    "y_valid = np.array(data_valid.iloc[:,1])\n",
    "y_valid = np.reshape(y_valid, (len(y_valid),1))\n",
    "\n",
    "data_test = pd.read_csv('https://raw.githubusercontent.com/cclin130/mais-202-assignment-2-f2019/master/Dataset_1_test.csv', header=None).sort_values(0)\n",
    "X_test = np.array(data_test.iloc[:,0])\n",
    "X_test = np.reshape(X_test, (len(X_test),1))\n",
    "y_test = np.array(data_test.iloc[:,1])\n",
    "y_test = np.reshape(y_test, (len(y_test),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5hufMCWAx-U"
   },
   "source": [
    "Let's take a look at what our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJ2ijROHAx-V"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmrLIXbiAx-b"
   },
   "source": [
    "## 2) Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPa8GRh4Ax-b"
   },
   "source": [
    "**This is a reminder that for this assignment, while you may use general utility libraries like numpy and pandas (specifically their matrix computations and data manipulations), pre-existing implementations of the model is prohibited.**\n",
    "\n",
    "### Q1: linear regression via closed-form ordinary least square solution\n",
    "\n",
    "Now we will implement polynomial regression for a 16-degree polynomial. As we saw in lectures, linear regression aims to find a solution to the equation:\n",
    "\n",
    "$$Y(X) = W^T \\cdot \\phi(X)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\phi(X) = \\left[  1 ,  X ,  X^2 ,  X^3 , \\ldots, X^n \\right ]$$\n",
    "\n",
    "The model does this by \"tweaking\" (aka training) W to maximize the probability function:\n",
    "\n",
    "$$p\\left(y \\;\\middle|\\; W^{\\operatorname T} x\\right)$$\n",
    " \n",
    "The closed-form ordinary least square solution to this problem (found by setting the gradient to 0) is:\n",
    "\n",
    "$$W = \\left(X^{\\operatorname T} X\\right)^{-1}X^{\\operatorname T}Y$$\n",
    "\n",
    "Now that we have the building blocks for linear regression, we can code it.\n",
    "\n",
    "*Note: Make sure to review the slides, do some research, and/or ask for clarification if this doesn't make sense. You must understand the underlying math before being able to implement this properly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8yNl77PAx-c"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Create the polynomial matrix Ï•(X) for the train, valid, and test sets\n",
    "\n",
    "X_train_poly = \n",
    "X_valid_poly = \n",
    "X_test_poly = \n",
    "\n",
    "### YOUR CODE HERE - Calculate the weighted matrix, save to variable 'W'\n",
    "# hint: this is your \"training\" phase, so you should only use X_train_poly and y_train\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE HERE - Make predictions on the training set and calculate the mean squared error.\n",
    "# make training set predictions, save to variable 'y_train_pred'\n",
    "\n",
    "y_train_pred = \n",
    "\n",
    "# calculate mean squared error, save to variable 'mse_train'\n",
    "\n",
    "mse_train = \n",
    "\n",
    "print(\"Training set Mean Squared Error: {}\".format(mse_train))\n",
    "\n",
    "### YOUR CODE HERE - Make predictions on the validation set and calculate the mean squared error.\n",
    "# make validation set predictions, save to variable 'y_valid_pred'\n",
    "y_valid_pred = \n",
    "\n",
    "# calculate mean squared error, save to variable 'mse_valid'\n",
    "mse_valid = \n",
    "\n",
    "print(\"Validation set Mean Squared Error: {}\".format(mse_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTsC7uroAx-f"
   },
   "outputs": [],
   "source": [
    "# plot training set using weights\n",
    "function = np.poly1d(np.flip(W[:, 0], 0)) \n",
    "x_axis = np.linspace(min(X_train), max(X_train))\n",
    "plt.plot(x_axis, function(x_axis), 'r', label='20 Degree Polynomial Fit')\n",
    "plt.scatter(X_train, y_train, s=10, label='Training set')\n",
    "plt.xlabel(\"input x\")\n",
    "plt.ylabel(\"f(x) for dataset and model\")\n",
    "plt.title(\"Training Set and Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OANFdRrAx-i"
   },
   "source": [
    "You should be getting a train MSE of around 11.519 and a validation MSE of around 14527. Something to note here is how much larger the validation set error is than the train error. Keep this in mind as we continue with the assignment.\n",
    "\n",
    "### Q2: linear regression via gradient descent\n",
    "\n",
    "We will now implement the same polynomial regression from above, but using gradient descent! In ML, we can't always optimize our cost functions with a closed form solution like in question 1 (it's often too computationally expensive). Thankfully, optimization algorithms, one of which is gradient descent, can help us approximate the minimum of the cost function.\n",
    "\n",
    "Recall that the cost function for linear regression is:\n",
    "\n",
    "$$ J(W) = \\frac{1}{2m}\\cdot\\sum_{i=0}^{m} \\left(W^{\\operatorname T}x^{(i)} - y^{(i)}\\right)^2 $$\n",
    "\n",
    "Where _i_ represents the sample number out of a total of *m* samples. Notice that the second factor is the sum of the squared errors. The $\\frac{1}{m}$ is to calculate the mean of the squared errors, and the $\\frac{1}{2}$ is to make the gradient nicer.\n",
    "\n",
    "When we take the partial derivative of J(W) with respect to weight $W_j$, the jth parameter of vector _W_, we get:\n",
    "\n",
    "$$ \\frac{dJ}{dw_j} = \\frac{1}{m}\\sum_{i=0}^{m} \\left (W^{\\operatorname T}x^{(i)} - y^{(i)} \\right ) \\cdot x_j^{(i)} $$\n",
    "\n",
    "Where $x_j^{(i)}$ is the jth parameter of the vector $x^{(i)}$.\n",
    "\n",
    "The following is the gradient descent algorithm for linear regression:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $w_j$ in W:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>$ \\displaystyle w_j := w_j - \\alpha \\cdot \\frac{1}{m}\\sum_{i=0}^{m} \\left (W^Tx^{(i)} - y^{(i)} \\right ) \\cdot x_j^{(i)}$\n",
    "    \n",
    "We can run the gradient descent update for as many itertions as needed until the amount the gradients change each loop is negligible (less than a given _epsilon_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDwMF8ViAx-j"
   },
   "outputs": [],
   "source": [
    "# we start by defining the relevant constants\n",
    "learning_rate = 0.55\n",
    "epsilon = 0.0003\n",
    "\n",
    "# weight matrix will be 16x1\n",
    "# we initialize the weights at 0\n",
    "W = np.zeros((16, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLcNYPe2Ax-n"
   },
   "source": [
    "To implement the gradient descent algorithm, we will need:\n",
    "1. a function that calculates the gradients of J (the cost function), with respect to each entry in W\n",
    "2. a function that calculates the change in the values of W after each gradient descent update\n",
    "3. a while loop that performs gradient descent until the change in W < epsilon\n",
    "\n",
    "*Note: training might take one or two minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyWmX2yVAx-p",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function that calculates the gradient\n",
    "def calculate_grad(X_poly, y, W):\n",
    "    # let dW store dJ/dW\n",
    "    dW = np.zeros((16,1))\n",
    "    m = len(X_poly)\n",
    "    y_pred = np.matmul(X_poly, W)\n",
    "    \n",
    "    for j, w_j in enumerate(W):\n",
    "      \n",
    "        ### YOUR CODE HERE - Calculate dW[j]\n",
    "        # Hint: You might find 'y_pred' helpful\n",
    "        # Hint: There is a way to do this without for-looping through every row (sample) in X_train_poly\n",
    "        # (And if you're having trouble, you can always go to office hours)\n",
    "        \n",
    "        dW[j] = \n",
    "        \n",
    "        ### ------------------------------\n",
    "        \n",
    "    return dW\n",
    "\n",
    "# function that caculates the change in W\n",
    "def calculate_dist(W_prev, W_cur):\n",
    "    return np.sqrt(np.sum((W_cur - W_prev)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMYKNwZ1TUWZ"
   },
   "outputs": [],
   "source": [
    "def train_polynomial_regression(X_train_poly, y_train, W, learning_rate, epsilon):\n",
    "  epoch_count = 0\n",
    "  while True:\n",
    "      #calculate current gradient\n",
    "      dW = calculate_grad(X_train_poly, y_train, W)\n",
    "\n",
    "      W_prev = W.copy()\n",
    "\n",
    "      ### YOUR CODE HERE - update each W[j] using the given learning_rate\n",
    "\n",
    "      \n",
    "\n",
    "      ### ------------------------------\n",
    "\n",
    "      diff = calculate_dist(W_prev, W)\n",
    "      if (diff < epsilon):\n",
    "          break\n",
    "\n",
    "      epoch_count +=1\n",
    "      # print train error every 50 iterations\n",
    "      if epoch_count % 20000 == 0:\n",
    "        y_train_pred = np.matmul(X_train_poly, W)\n",
    "        print('Training set Mean Squared Error: {}'.format(np.power((y_train_pred - y_train), 2).mean()))\n",
    "  \n",
    "  print('Training complete.')\n",
    "  return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSr1uNbXTpzg"
   },
   "outputs": [],
   "source": [
    "W = train_polynomial_regression(X_train_poly, y_train, W, learning_rate, epsilon)\n",
    "\n",
    "#calculated MSE\n",
    "y_valid_pred = np.matmul(X_valid_poly, W)\n",
    "mse_valid = np.power((y_valid_pred - y_valid), 2).mean()\n",
    "print('\\nValidation set Mean Squared Error: {}'.format(mse_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAaN9IktAx_X"
   },
   "source": [
    "Great job! You just implemented polynomial regression in two different ways. Let's now plot the results of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veAWJ3qIAx_Y"
   },
   "outputs": [],
   "source": [
    "# plot training set using weights from gradient descent\n",
    "function = np.poly1d(np.flip(W[:, 0], 0)) \n",
    "x_axis = np.linspace(min(X_train), max(X_train))\n",
    "plt.plot(x_axis, function(x_axis), 'r', label='20 Degree Polynomial Fit')\n",
    "plt.scatter(X_train, y_train, s=10, label='Training set')\n",
    "plt.xlabel(\"input x\")\n",
    "plt.ylabel(\"f(x) for dataset and model\")\n",
    "plt.title(\"Training Set and Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGXVDA9TAx_a"
   },
   "outputs": [],
   "source": [
    "# plot validation set using weights from gradient descent\n",
    "function = np.poly1d(np.flip(W[:, 0], 0)) \n",
    "x_axis = np.linspace(min(X_valid), max(X_valid))\n",
    "plt.plot(x_axis, function(x_axis), 'r', label=\"15 Degree Polynomial Fit\")\n",
    "plt.scatter(X_valid, y_valid, s=10, label='Validation set')\n",
    "plt.xlabel(\"input x\")\n",
    "plt.ylabel(\"f(x) for dataset and model\")\n",
    "plt.title(\"Validation Set and Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKeqX2vgAx_d"
   },
   "source": [
    "As you can see again, the MSE of the validation set is much higher than that of the training set; in other words, our model doesn't perform that well on the validation set. You may have also noticed that the results from gradient descent do not perfectly match the results from calculating _W_ using the closed-form solution. Given our low number of data points, in this case finding the minimum of the cost function with gradient descent takes longer than using the approach from Q1. (If you want to get the exact results, just run the training cell again and keep iterating--but it might take a while.)\n",
    "\n",
    "### Q3: Using your knowledge from the lectures, explain whether the model is overfitting or underfitting the training data and a potential reason why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rIPgWFCAx_e"
   },
   "source": [
    "\\### YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQwimW8mAx_e"
   },
   "source": [
    "## 3) Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCqVvsNsSmSo"
   },
   "source": [
    "### Q4: regularized linear regression via gradient descent\n",
    "\n",
    "One way to address overfitting is to add regularization. In this part of the assignment, we will be adding *L2 regularization* to gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BgSi8j4a29-"
   },
   "source": [
    "Recall that with regularization, the Residual Sum of Squares equation becomes:\n",
    "\n",
    "$$ RSS(W) =  \\sum_{i=0}^{m} \\left (W^{\\operatorname T}x^{(i)} - y^{(i)} \\right )^2 + \\lambda \\cdot \\sum_{j=1}^{p} w_j^2 $$\n",
    "\n",
    "Where _i_ represents the sample number out of a total of *m* samples and $w_j$ represents the jth parameter of W out of _p_ parameters. The reason j starts at 1 is because we normally don't regularize the bias term $w_0$.\n",
    "\n",
    "Making these same changes to our cost function from Q2, we have:\n",
    "\n",
    "$$ J(W) = \\frac{1}{2m}\\left[\\sum_{i=0}^{m} \\left (W^{\\operatorname T}x^{(i)} - y^{(i)} \\right )^2 + \\lambda \\cdot \\sum_{j=1}^{p} w_j^2\\right] $$\n",
    "\n",
    "Once again, $\\frac{1}{m}$ is to calculate the mean of the squared errors, and the $\\frac{1}{2}$ is to make the gradient nicer.\n",
    "\n",
    "Now, when we take the partial derivative of J(W) with respect to weight $w_j$, the jth parameter of vector _W_, we get a different result for $w_0$ than for the rest of the parameters:\n",
    "\n",
    "$$ \\frac{dJ}{dw_0} = \\frac{1}{m}\\sum_{i=1}^{m} \\left (W^{\\operatorname T}x^{(i)} - y^{(i)}\\right) \\cdot x_j^{(i)} $$\n",
    "\n",
    "$$ \\frac{dJ}{dw_j} = \\frac{1}{m}\\left(\\sum_{i=1}^{m} \\left(W^{\\operatorname T}x^{(i)} - y^{(i)}\\right) \\cdot x_j^{(i)}\\right) + \\frac{\\lambda}{m} \\cdot w_j  \\quad \\text{for}\\ j = 1, 2, \\ldots, p $$\n",
    "\n",
    "Putting this all together into the gradient descent algorithm for regularized linear regression gives us:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $w_j$ in W:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>if j = 0<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>$ \\displaystyle w_j := w_j - \\alpha \\cdot \\frac{1}{m}\\sum_{i=1}^{m} (W^Tx^{(i)} - y^{(i)}) \\cdot x_j^{(i)}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>else<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>$ \\displaystyle w_j := w_j - \\alpha \\cdot \\left (\\left (\\frac{1}{m}\\sum_{i=1}^{m} \\left(W^Tx^{(i)} - y^{(i)}\\right) \\cdot x_j^{(i)}\\right) + \\frac{\\lambda}{m} \\cdot w_j \\right ) $<br>\n",
    "    \n",
    "We can run the gradient descent update for as many itertions as needed until the amount the gradients change each loop is negligible (less than a given _epsilon_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyAFBAinAx_f"
   },
   "outputs": [],
   "source": [
    "# we start by defining the relevant constants (the same as Q2)\n",
    "learning_rate = 0.55\n",
    "epsilon = 0.0003\n",
    "lambda_value = 0.3\n",
    "\n",
    "# weight matrix will be 16x1\n",
    "# we initialize the weights at 0\n",
    "W = np.zeros((16, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvIlRGC_UKZt"
   },
   "source": [
    "To implement the gradient descent algorithm with regularization, we will need:\n",
    "1. a function that calculates the gradients of J (the cost function including regularization terms), with respect to each entry in W\n",
    "2. a function that calculates the change in the values of W after each gradient descent update\n",
    "3. a while loop that performs gradient descent until the change in W < epsilon\n",
    "\n",
    "*Note: training might take one or two minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LObgqJd8Ax_i"
   },
   "outputs": [],
   "source": [
    "# function that calculates the gradient\n",
    "def calculate_regularized_grad(X_poly, y, W, lambda_value):\n",
    "    # let dW store dJ/dW\n",
    "    dW = np.zeros((16,1))\n",
    "    m = len(X_poly)\n",
    "    y_pred = np.matmul(X_poly, W)\n",
    "    \n",
    "    for j, w_j in enumerate(W):\n",
    "        ### YOUR CODE HERE - Calculate dW[j]\n",
    "        # Hint: You can just copy your implementation from Q2\n",
    "        # then append the L2 regularization term at the end\n",
    "        \n",
    "        dW[j] = \n",
    "        \n",
    "        ### ------------------------------\n",
    "        \n",
    "    return dW\n",
    "\n",
    "# function that caculates the change in W\n",
    "def calculate_dist(W_prev, W_cur):\n",
    "    return np.sqrt(np.sum((W_cur - W_prev)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxU2yedkT8hx"
   },
   "outputs": [],
   "source": [
    "def train_regularized_polynomial_regression(X_train_poly, y_train, W, learning_rate, epsilon, lambda_value, verbose=True):\n",
    "  epoch_count = 0\n",
    "  while True:\n",
    "      #calculate current gradient\n",
    "      dW = calculate_regularized_grad(X_train_poly, y_train, W, lambda_value)\n",
    "\n",
    "      W_prev = W.copy()\n",
    "\n",
    "      ### YOUR CODE HERE - update W[j] using the given learning_rate\n",
    "      # Hint: This should be the same as your implementation from Q2\n",
    "\n",
    "\n",
    "\n",
    "      ### ------------------------------\n",
    "\n",
    "      diff = calculate_dist(W_prev, W)\n",
    "      if (diff < epsilon):\n",
    "          break\n",
    "\n",
    "      epoch_count +=1\n",
    "      # print train error every 50 iterations\n",
    "      if verbose:\n",
    "        if epoch_count % 100 == 0:\n",
    "          y_train_pred = np.matmul(X_train_poly, W)\n",
    "          print('Training set Mean Squared Error: {}'.format(np.power((y_train_pred - y_train), 2).mean()))\n",
    "\n",
    "  print('Training complete.')\n",
    "  return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPmjg2kOT-3_"
   },
   "outputs": [],
   "source": [
    "W = train_regularized_polynomial_regression(X_train_poly, y_train, W, learning_rate, epsilon, lambda_value)\n",
    "#calculated MSE\n",
    "y_valid_pred = np.matmul(X_valid_poly, W)\n",
    "mse_valid = np.power((y_valid_pred - y_valid), 2).mean()\n",
    "print('\\nValidation set Mean Squared Error: {}'.format(mse_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FJ4E2DKVq6N"
   },
   "source": [
    "Now we plot our training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ct_ZQp87Vq6P"
   },
   "outputs": [],
   "source": [
    "# plot training set using weights from gradient descent\n",
    "function = np.poly1d(np.flip(W[:, 0], 0)) \n",
    "x_axis = np.linspace(min(X_train), max(X_train))\n",
    "plt.plot(x_axis, function(x_axis), 'r', label='15 Degree Polynomial Fit')\n",
    "plt.scatter(X_train, y_train, s=10, label='Training set')\n",
    "plt.xlabel(\"input x\")\n",
    "plt.ylabel(\"f(x) for dataset and model\")\n",
    "plt.title(\"Training Set and Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3gcua70Vq6U"
   },
   "outputs": [],
   "source": [
    "# plot validation set using weights from gradient descent\n",
    "function = np.poly1d(np.flip(W[:, 0], 0)) \n",
    "x_axis = np.linspace(min(X_valid), max(X_valid))\n",
    "plt.plot(x_axis, function(x_axis), 'r', label=\"15 Degree Polynomial Fit\")\n",
    "plt.scatter(X_valid, y_valid, s=10, label='Validation set')\n",
    "plt.xlabel(\"input x\")\n",
    "plt.ylabel(\"f(x) for dataset and model\")\n",
    "plt.title(\"Validation Set and Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_WfFLNIVyDn"
   },
   "source": [
    "As you can see, this new model generalizes better to our validation set. However, while it does well, we don't really know if this is the absolute most-generalizable model we can create, since we chose the *lambda_value* pretty arbitrarily. We need to tune lambda (aka how much we penalize large weights) to decrease overfitting as much as possible.\n",
    "\n",
    "This is where our validation set comes in. We already know that our training algorithm works, but we need to tune lambda via trial-and-error to optimize for a model that performs the best on unseen data. We choose to use a validation set instead of our test set because we need completely new test set data to obtain a truly fair performance metric at the end. To reiterate, hyperparmaeter-tuning with the validation set means that our model is \"fit\" (to some extent) to the cross validation data, so measuring performance on the validation set gives our model an unfair advantage.\n",
    "\n",
    "Run the following cell to loop through a list of potential lambda values to find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWD2ENpGTMJ5"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.55\n",
    "epsilon = 0.0003\n",
    "cross_validation_weights = []\n",
    "cross_validation_MSE = []\n",
    "lambda_list = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "\n",
    "for lambda_value in lambda_list:\n",
    "  W = np.zeros((16, 1))\n",
    "  W = train_regularized_polynomial_regression(X_train_poly, y_train, W,\n",
    "                                              learning_rate, epsilon,\n",
    "                                              lambda_value, verbose=False)\n",
    "  \n",
    "  #calculated MSE\n",
    "  y_valid_pred = np.matmul(X_valid_poly, W)\n",
    "  mse_valid = np.power((y_valid_pred - y_valid), 2).mean()\n",
    "  print('\\nValidation set MSE for {0} lambda: {1}\\n'.format(lambda_value, mse_valid))\n",
    "  \n",
    "  cross_validation_weights.append(W)\n",
    "  cross_validation_MSE.append(mse_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVSOOmf_XyVc"
   },
   "source": [
    "Let's plot the results from our cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Gi2gZ7cXw8H"
   },
   "outputs": [],
   "source": [
    "plt.plot(lambda_list, cross_validation_MSEs)\n",
    "plt.xlabel('lambda_value')\n",
    "plt.ylabel('Validation set MSE')\n",
    "plt.title(\"Cross validation error vs. lambda\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPtKJmpEdqNL"
   },
   "source": [
    "We can now pick the best weight from the ones we've trained, and then measure model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLSH5zE6Zaqs"
   },
   "outputs": [],
   "source": [
    "lambda_value = lambda_list[np.argmin(cross_validation_MSEs)]\n",
    "print('Best lambda: {}'.format(lambda_value))\n",
    "\n",
    "# pick best weight\n",
    "W = cross_validation_weights[np.argmin(cross_validation_MSEs)]\n",
    "\n",
    "# calculate MSE on test set\n",
    "y_test_pred = np.matmul(X_test_poly, W)\n",
    "mse_test = np.power((y_test_pred - y_test), 2).mean()\n",
    "print('\\nTest set MSE: {}\\n'.format(mse_test))\n",
    "\n",
    "# plot the results\n",
    "function = np.poly1d(np.flip(W[:, 0], 0)) \n",
    "x_axis = np.linspace(min(X_test), max(X_test))\n",
    "plt.plot(x_axis, function(x_axis), 'r', label=\"15 Degree Polynomial Fit\")\n",
    "plt.scatter(X_test, y_test, s=10, label='Validation set')\n",
    "plt.xlabel(\"input x\")\n",
    "plt.ylabel(\"f(x) for dataset and model\")\n",
    "plt.title(\"Test Set and Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LygvqAhAx_j"
   },
   "source": [
    "### Q5: What do you think is the degree of the source polynomial? What evidence in the previous cells indicate that regularization addressed the issues (overfitting/underfitting) mentioned in Q3? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bJtFp90Ax_k"
   },
   "source": [
    "\\### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLks2ssYAx_l"
   },
   "source": [
    "#### This is the end of your first MAIS 202 assignment. To submit this assignment, download it as a python file and upload it to its respective okpy assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
